<html>
  <head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>CSC 4851/6851: Intro to Deep Learning</title>
  </head>

  <body bgcolor="#55555" alink="blue" vlink="navy">

  <center>
  <table width=80% cellpadding=10 bgcolor="white"><tr><td>
  
  <!-- COURSE TITLE -->
  <table border="0" cellpadding="0" width="100%" style="width: 100%;">
      <tbody>
       <tr>
       <td style="padding: 0.75pt;" width="80" align="center">
       <p class="MsoNormal">&nbsp;<img width="56" height="56" src="./course_files/gsu_logo.jpg" alt="gsu_logo"></p>
       </td>
       <td style="padding: 0.75pt;">
       <p><span style="font-size: 18pt;">
       <b><big>&nbsp;CSC 4851/6851<br>
       &nbsp;Intro to Deep Learning</big></b><br>
       </span></p>
       </td>
       </tr>
      </tbody>
  </table>

  <hr size="2" width="100%" align="center">

  <!-- COURSE SCHEDULE -->
  <table border="0" cellpadding="0" width="100%" bgcolor="#1F618D" style="background: rgb(31,97,141) none repeat scroll 50% 0%; width: 100%;">
    <tbody>
      <tr>
       <td style="padding: 2.25pt;">
       <p class="MsoNormal"><b><span style="font-size: 13.5pt; color: white;">Course Schedule</span></b></p> 
       </td>
      </tr>
    </tbody>
  </table>

  <p>(Tentative schedule for the course and deviations may be necessary as the course progresses.)</p>
  
  <table border="1" width="100%">
	<tr>
		<th>Date</th>
		<th>Topic</th>
		<th>Notes</th>
		<th>Reading (Textbook or Other Materials)</th>
	</tr>
	<tr>
		<td>Jan. 11</td>
		<td align=center>Introduction</td>
		<td></td>
		<td><a href="http://www.deeplearningbook.org/contents/intro.html">Chapter 1, 2, 3</a>, <a href="https://www.nature.com/articles/nature14539">Nature review on DL</a>, <a href="http://www.stat.wisc.edu/~ifischer/calculus.pdf">Basic calculus refresher</a></td>
	</tr>
	<tr>
		<td>Jan. 13</td>
		<td align=center>Machine Learning Basics</td>
		<td></td>
		<td><a href="http://www.deeplearningbook.org/contents/numerical.html">Chapter 4, 5</a>, <a href="http://courses.washington.edu/css490/2012.Winter/lecture_slides/02_math_essentials.pdf"> Machine learning math essentials</a></td>
	</tr>
	<tr>
		<td>Jan. 20</td>
		<td align="center">
		Shallow Classifiers</td>
		<td><center>Project Details</center></td>
		<td><a href="http://www.deeplearningbook.org/contents/numerical.html">Chapter 4, 5</a></td>
	</tr>
	<tr>
		<td>Jan. 25</td>
		<td align="center">
		Loss Functions</td>
		<td><center></center></td>
		<td><a href="http://www.deeplearningbook.org/contents/numerical.html">Chapter 4, 5</a></td>
	</tr>
        <tr>
		<td>Jan. 27</td>
		<td><center>Optimization, SGD</center></td>
                <td><center>Set up Python IDE</center></td>
		<td><a href="http://www.deeplearningbook.org/contents/numerical.html">Chapter 4, 5</a></td>
	</tr>
        <tr>
                <td>Feb. 1</td>
                <td><center>Tutorial on Python and Numpy</center></td>
                <td><center></center></td>
                <td></td>
        </tr>
	<tr>
		<td>Feb. 3</td>
		<td align="center">
		Backpropagation</td>
		<td></td>
		<td><a href="http://www.deeplearningbook.org/contents/numerical.html">Chapter 4, 5</a></td>
	</tr>
<!--	<tr>
		<td>Feb. 10</td>
		<td align="center">
		Intro to Neural Networks</td>
		<td></td>
		<td></td>
	</tr>
	<tr>
		<td>Feb. 12</td>
		<td align="center">
		Intro to Convolutional Neural Networks, Convolution</td>
		<td></td>
                <td><a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks">AlexNet</a></td>
	</tr>
        <tr>
                <td>Feb. 17</td>
                <td align="center">
                Pooling, Demo, BP example</td>
                <td></td>
                <td><a href="https://arxiv.org/abs/1412.6806">All Convolutional Net</a></td>
        </tr>
	<tr>
		<td>Feb. 19</td>
		<td align="center">Training Neural Networks</td>
		<td></td>
		<td></td>
	</tr>
        <tr>
                <td>Feb. 24</td>
                <td align="center">Training Neural Networks II, Batch Normalization</td>
                <td></td>
                <td><a href="https://arxiv.org/abs/1502.03167">Batch Normalization</a></td>
        </tr>
	<tr>
		<td>Feb. 26</td>
		<td align="center">Second-order Optimization</td>
		<td></td>
		<td><a href="http://www.deeplearningbook.org/contents/optimization.html">Chapter 8</a></td>
	</tr>
	<tr>
		<td>Mar. 2</td>
		<td align="center">Regularization, Transfer Learning</td>
		<td></td>
                <td><a href="http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf">Dropout</a>, <a href="https://arxiv.org/abs/1511.06909">Blackout</a>, <a href="https://arxiv.org/abs/1310.1531">DeCAF</a></td>
	</tr>
	<tr>
		<td>Mar. 4</td>
		<td align="center">ImageNet, CNN Architectures</td>
		<td></td>
		<td><a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">AlexNet</a>, <a href="https://arxiv.org/pdf/1409.1556v6.pdf">VGGNet</a>, <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf">GoogleNet</a>, <a href="https://arxiv.org/abs/1512.03385">ResNet</a></td>
	</tr>
	<tr>
		<td>Mar. 9</td>
		<td align="center">CNN Architectures and Visualization</td>
		<td></td>
		<td><a href="https://arxiv.org/abs/1605.07146">Wide ResNet</a>, <a href="https://arxiv.org/abs/1311.2901">Zeiler and Fergus</a>, <a href="https://arxiv.org/abs/1506.06579">Yosinski et al.</a></td>
	</tr>
	<tr>
	  <td>Mar. 11</td>
	  <td align="center">Word2Vec</td>
	  <td><center></center></td>
	  <td><a href="https://arxiv.org/pdf/1310.4546.pdf">word2vec</a>, <a href="https://nlp.stanford.edu/pubs/glove.pdf">glove</a>, <a href="https://arxiv.org/abs/1506.02761">wordrank</a></td>
        </tr>
	<tr>
	  <td>Mar. 30</td>
	  <td align="center">Doc2Vec, RNNLM</td>
	  <td></td>
	  <td><a href="https://cs.stanford.edu/~quocle/paragraph_vector.pdf">doc2vec</a>, <a href="http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf">rnnlm</a></td>
        </tr>
	<tr>
	  <td>Apr. 1</td>
	  <td align="center">LSTM, GRU</td>
	  <td></td>
	  <td><a href="http://www.deeplearningbook.org/contents/rnn.html">Chapter 10</a></td>
        </tr>
	<tr>
	  <td>Apr. 6</td>
	  <td align="center">Image Caption, Machine Translation</td>
	  <td></td>
	  <td><a href="https://arxiv.org/abs/1502.03044">Show, Attend and Tell</a>, <a href="https://arxiv.org/abs/1409.0473">NMT with Attention</a></td>
        </tr>
	<tr>
	  <td>Apr. 8</td>
	  <td align="center">Unsupervised Learning, PCA, AutoEncoder</td>
	  <td></td>
	  <td><a href="http://www.deeplearningbook.org/contents/linear_factors.html">Chapter 13, 14</a></td>
        </tr>
	<tr>
	  <td>Apr. 13</td>
	  <td align="center">Variational AutoEncoder</td>
	  <td></td>
	  <td><a href="https://arxiv.org/abs/1312.6114">VAE</a></td>
        </tr>
	<tr>
	  <td>Apr. 15</td>
	  <td align="center">Generative Adversarial Networks</td>
	  <td></td>
	  <td><a href="https://arxiv.org/abs/1406.2661">GANs</a>, <a href="https://arxiv.org/abs/1511.06434">DCGANs</a></td>
        </tr>
        <tr>
          <td>Apr. 20</td>
          <td align="center">Review (office hours)</td>
          <td align="center"></td>
          <td></td>
        </tr>
        <tr>
          <td>Apr. 22</td>
          <td align="center">Final Exam</td>
          <td></td>
          <td></td>
        </tr>
	<tr>
	  <td>Apr. 27</td>
	  <td align="center">Presentation</td>
	  <td></td>
	  <td></td>
        </tr>
	<tr>
	  <td>Apr. 29</td>
	  <td align="center">Presentation</td>
	  <td></td>
	  <td></td>
        </tr>
	<tr>
	  <td>May 4</td>
	  <td align="center">Presentation</td>
	  <td></td>
	  <td></td>
        </tr>
	<tr>
	  <td>May 6</td>
	  <td align="center"><strong>Project Report Due (by 11:59 pm)</strong></td>
	  <td></td>
	  <td></td>
        </tr> -->
	</table>
	<p>&nbsp;</p>
        <!--<p>&nbsp;</p>-->
    <a href="index.html">back to course page</a>
	</td></tr></table></center>
</body>
</html>
